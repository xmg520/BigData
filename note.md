## 1. 数据收集

### 1.1 互联网数据收集

#### 网络爬虫

###### 特点

- 深度优先

- 广（宽）度优先

###### 开源系统

- Apache Nutch

  > 一个开源的基于java实现的搜索引擎
  >
  > Apache Lucene 一个开放源代码的全文搜索引擎

- Heritrix 一个开源的java实现的网络爬虫



### 1.2 内部数据收集

#### 1.2.1 Apache Flume

> 一个分布式、可靠和高可用的海量数据收集系统

###### 核心模块

- Source
- Sink
- Channel
- Agent
- Event

#### 1.2.2 Facebook scribe

> 实时分布式数据收集系统

#### 1.2.3 Logstash

> 作为ELK(Elasticsearch Logstash Kibana)技术栈之一使用
>
> 用于日志的收集和处理



## 2.数据存储

### 2.1持久化存储

2.1.1 Hadoop和HDFS

###### Hadoop优势

- 透明性
- 高扩展性
- 高效性
- 高容错和高可靠性
- 低成本

###### Hadoop核心组件

- HDFS

> 分布式文件系统，前身是GFS
>
> 不适合实时性很强的数据访问
>
> 无法高效存储大量小文件
>
> 磁盘IO开销大

- MapReduce

#### 2.1.2 HBase

> 分布式面向列的开源数据库

###### 主要元素

- HMaster
- HRegion
- HRgionServer

#### 2.1.3 MongoDB

> 分布式文件存储的数据库
>
> 非关系型数据库中最像关系型数据库的数据库
>
> 良好的并行处理和高扩展性
>
> 支持数据复制和恢复
>
> 实现故障恢复



### 2.2 非持久化存储

#### 2.2.1 缓存和散列

#### 2.2.2 Memcached

> 高性能分布式内存对象缓存系统

#### 2.2.3 BerkeleyDB

> 历史悠久的嵌入式数据库系统

#### 2.2.4 Redis

> 远程字典服务器
>
> 一个开源的高性能的基于键-值型的缓存和存储系统

###### 特性

- 极高的性能
- 支持多种数据类型
- 支持事务性
- 可设定生命周期
- 提供持久化存储



## 3.数据处理

### 3.1 离线批量处理

3.1.1 MapReduce

3.1.2 Spark

3.1.3 Hive

> 可以存储、查询、分析存储在HDFS中的大规模数据
>
> 将SQL查询转化为MapReduce任务

3.1.4 Pig

> SQL的一个面向过程的简化版本

3.1.5 Impala

> 面对实时性更强的需求
>
> 适合处理输出数据较小的查询请求

3.1.6 Spark SQL

### 3.2 提升及时性：消息机制

3.2.1 ActiveMQ

> Apache出品的消息系统

#### 3.2.2 Apache Kafka

> LinkedIn设计开发的高吞吐量的分布式发布订阅消息系统

###### 特点

- 高性能存储
- 天生分布式
- 高吞吐量
- 无状态代理
- 主题和分区
- 消费者分组

### 3.3 在线实时处理

#### 3.3.1 Storm

> 源于twitter
>
> 分布式容错的实时计算系统

#### 3.3.2 Spark Streaming

> 一个实时数据进行快速流式处理的系统
>
> 基本原理：按照很小的时间段对流式数据进行切分，然后以类似于批处理的方式来处理这一小段的数据。



###### 名词解释

- ELK

> ELK是Elasticsearch、Logstash、Kibana的简称，这三者是核心套件，但并非全部。

> Elasticsearch是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能；是一套开放REST和JAVA API等结构提供高效搜索功能，可扩展的分布式系统。它构建于Apache Lucene搜索引擎库之上。

> Logstash是一个用来搜集、分析、过滤日志的工具。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。它可以从许多来源接收日志，这些来源包括 syslog、消息传递（例如 RabbitMQ）和JMX，它能够以多种方式输出数据，包括电子邮件、websockets和Elasticsearch。

>  Kibana是一个基于Web的图形界面，用于搜索、分析和可视化存储在 Elasticsearch指标中的日志数据。它利用Elasticsearch的REST接口来检索数据，不仅允许用户创建他们自己的数据的定制仪表板视图，还允许他们以特殊的方式查询和过滤数据

- ETL

> ETL是将业务系统的数据经过抽取、清洗转换之后加载到数据仓库的过程，目的是将企业中的分散、零乱、标准不统一的数据整合到一起，为企业的决策提供分析依据。** ETL是BI项目重要的一个环节。 通常情况下，在BI项目中ETL会花掉整个项目至少1/3的时间,ETL设计的好坏直接关接到BI项目的成败。       

> 　　ETL的设计分三部分：数据抽取、数据的清洗转换、数据的加载。在设计ETL的时候我们也是从这三部分出发。数据的抽取是从各个不同的数据源抽取到ODS(Operational Data Store，操作型数据存储)中——这个过程也可以做一些数据的清洗和转换)，在抽取的过程中需要挑选不同的抽取方法，尽可能的提高ETL的运行效率。ETL三个部分中，花费时间最长的是“T”(Transform，清洗、转换)的部分，一般情况下这部分工作量是整个ETL的2/3。数据的加载一般在数据清洗完了之后直接写入DW(Data Warehousing，数据仓库)中去。

> 　　ETL的实现有多种方法，常用的有三种。一种是借助ETL工具(如Oracle的OWB、SQL Server 2000的DTS、SQL Server2005的SSIS服务、Informatic等)实现，一种是SQL方式实现，另外一种是ETL工具和SQL相结合。前两种方法各有各的优缺点，借助工具可以快速的建立起ETL工程，屏蔽了复杂的编码任务，提高了速度，降低了难度，但是缺少灵活性。SQL的方法优点是灵活，提高ETL运行效率，但是编码复杂，对技术要求比较高。第三种是综合了前面二种的优点，会极大地提高ETL的开发速度和效率。

- BI

> 商业智能（Business Intelligence，简称：BI）**，又称**商业智慧**或**商务智能**，指用现代[数据仓库](https://baike.baidu.com/item/数据仓库)技术、[线上分析处理](https://baike.baidu.com/item/线上分析处理)技术、[数据挖掘](https://baike.baidu.com/item/数据挖掘)和数据展现技术进行数据分析以实现[商业价值](https://baike.baidu.com/item/商业价值)。

> 商业智能作为一个工具，是用来处理企业中现有数据，并将其转换成知识、分析和结论，辅助业务或者决策者做出正确且明智的决定。是帮助企业更好地利用数据提高决策质量的技术，包含了从数据仓库到分析型系统等。

> 商业智能系统，通常简称为商业智能系统，是商业智能软件的简称，是为提高企业经营绩效而采用的一系列方法、技术和软件的总和。通常被理解为将企业中的现有数据转换为知识并帮助企业做出明智的业务决策的工具。